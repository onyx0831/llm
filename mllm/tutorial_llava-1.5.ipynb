{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f8d81e1-f587-44c5-a5c2-d429c3923d01",
   "metadata": {},
   "source": [
    "LLaVA-1.6をnotebookで動かす方法は現状、こちらのYoutubeしか見つからない\n",
    "- https://www.youtube.com/watch?v=SEavari8xaU\n",
    "\n",
    "LLaVA-1.5であれば、HuggingFaceがサポートしているのですぐ動かせそう\n",
    "- https://colab.research.google.com/drive/1qsl6cd2c8gGtEW1xV5io7S8NHh-Cp1TV?usp=sharing#scrollTo=DFVZgElEQk3x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c611e04-cae3-4954-ac5c-e15ff2ab1c0e",
   "metadata": {},
   "source": [
    "### LLaVA-1.5を動かす"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca18b00-808c-4b8c-ab95-99b8a9eced7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import pipeline\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from src.utils import download_image_from_s3\n",
    "\n",
    "openai.api_key = ''\n",
    "\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_compute_dtype=torch.float16\n",
    "# )\n",
    "\n",
    "model_id = \"/media/sj-archimedes/data/03_pretrained_model/llm/llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"image-to-text\",\n",
    "    model=model_id,\n",
    "    # device='cuda:1',\n",
    "    device_map='auto',\n",
    "    # model_kwargs={\"quantization_config\": quantization_config}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044d9b0a-0edd-4b53-9f37-71073fd7e2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_url = \"https://llava-vl.github.io/static/images/view.jpg\"\n",
    "# image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "# image\n",
    "\n",
    "df = pd.read_pickle('/media/sj-archimedes/data/share/OddAI_Library_practice/data08/lap_trainval_20221001-20231031.pkl')\n",
    "df = df.query('creative_type == \"image\"')\n",
    "df['creative_media_url'] = df['creative_media_url'].map(lambda x: x[0])\n",
    "df['creative_media_hash'] = df['creative_media_hash'].map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c9b3c4-cc89-4b39-b0db-55a2c17f359c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日本語で生成されるとうまくいかない例。このような例が散見されたため、日本語を直接出力する方法は断念する\n",
    "# df = df.query('creative_media_hash == \"QRBZ4C3P6UTEJR62P2XAGIDZ6Q\"')\n",
    "\n",
    "# s3_url = df['creative_media_url'].iloc[0]\n",
    "\n",
    "# img = download_image_from_s3(s3_url)\n",
    "# image_list = [img]\n",
    "# img\n",
    "s3_url = df['creative_media_url'].sample(3).tolist()\n",
    "print(s3_url)\n",
    "image_list = [download_image_from_s3(path) for path in s3_url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02f4f20-5f45-49f4-9ec4-68befa12eb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(1,3, figsize=(15,5))\n",
    "for i, img in enumerate(image_list):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59247e7-d660-4f16-9b9f-f34a4719f312",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "max_new_tokens = 200\n",
    "# prompt = \"USER: <image>\\nPlease describe this image in Japanese.\\nASSISTANT:\"\n",
    "# prompt = \"USER: <image>\\nこの画像を日本語で説明してください。\\nASSISTANT:\"\n",
    "# prompt = \"USER: <image>\\nPlease describe this image.\\nASSISTANT:\"\n",
    "prompt = \"USER: <image>\\nPlease explain in detail how this advertisement image has been designed to appear attractive to consumers.\\nASSISTANT:\"\n",
    "\n",
    "for img in image_list:\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    outputs = pipe(\n",
    "        img,\n",
    "        prompt=prompt,\n",
    "        generate_kwargs={\"max_new_tokens\": 512}\n",
    "    )\n",
    "    \n",
    "    response = outputs[0]['generated_text'].split('\\nASSISTANT: ')[1]\n",
    "    \n",
    "    print(response)\n",
    "    \n",
    "    instruction = f\"次の文章を日本語に翻訳してください。\\n{response}\"\n",
    "    \n",
    "    \n",
    "    client = OpenAI()\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": instruction},\n",
    "      ]\n",
    "    )\n",
    "    \n",
    "    response = response.choices[0].message.content\n",
    "    \n",
    "    print(response)\n",
    "    print('\\n==========\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f601176-b72a-4235-b881-faf52acf55a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "max_new_tokens = 200\n",
    "# prompt = \"USER: <image>\\nPlease describe this image in Japanese.\\nASSISTANT:\"\n",
    "prompt = \"USER: <image>\\nPlease describe this image.\\nASSISTANT:\"\n",
    "\n",
    "for img in image_list:\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    outputs = pipe(\n",
    "        img,\n",
    "        prompt=prompt,\n",
    "        generate_kwargs={\"max_new_tokens\": 200}\n",
    "    )\n",
    "    \n",
    "    response = outputs[0]['generated_text'].split('\\nASSISTANT: ')[1]\n",
    "    \n",
    "    print(response)\n",
    "    \n",
    "    instruction = f\"次の文章を日本語に翻訳してください。\\n{response}\"    \n",
    "    client = OpenAI()\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": instruction},\n",
    "      ]\n",
    "    )\n",
    "    \n",
    "    response = response.choices[0].message.content\n",
    "    \n",
    "    print(response)\n",
    "    print('\\n==========\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3d753a-e151-480a-abc3-9bf30f02a50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "max_new_tokens = 200\n",
    "prompt = \"USER: <image>\\nPlease describe this advertisement image.\\nASSISTANT:\"\n",
    "\n",
    "for img in image_list:\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    outputs = pipe(\n",
    "        img,\n",
    "        prompt=prompt,\n",
    "        generate_kwargs={\"max_new_tokens\": 200}\n",
    "    )\n",
    "    \n",
    "    response = outputs[0]['generated_text'].split('\\nASSISTANT: ')[1]\n",
    "    \n",
    "    print(response)\n",
    "    \n",
    "    instruction = f\"次の文章を日本語に翻訳してください。\\n{response}\"    \n",
    "    client = OpenAI()\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": instruction},\n",
    "      ]\n",
    "    )\n",
    "    \n",
    "    response = response.choices[0].message.content\n",
    "    \n",
    "    print(response)\n",
    "    print('\\n==========\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4becbbe-18e3-48c1-8d76-11ebe198949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "max_new_tokens = 200\n",
    "prompt = \"USER: <image>\\nPlease explain the layout representation of this advertisement image.\\nASSISTANT:\"\n",
    "\n",
    "for img in image_list:\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    outputs = pipe(\n",
    "        img,\n",
    "        prompt=prompt,\n",
    "        generate_kwargs={\"max_new_tokens\": 200}\n",
    "    )\n",
    "    \n",
    "    response = outputs[0]['generated_text'].split('\\nASSISTANT: ')[1]\n",
    "    \n",
    "    print(response)\n",
    "    \n",
    "    instruction = f\"次の文章を日本語に翻訳してください。\\n{response}\"    \n",
    "    client = OpenAI()\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": instruction},\n",
    "      ]\n",
    "    )\n",
    "    \n",
    "    response = response.choices[0].message.content\n",
    "    \n",
    "    print(response)\n",
    "    print('\\n==========\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbf4837-79db-43e6-8961-b4c6f82dd418",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "max_new_tokens = 200\n",
    "prompt = \"USER: <image>\\nPlease explain the layout techniques used, particularly regarding the material images and text layers in the advertisement image.\\nASSISTANT:\"\n",
    "\n",
    "for img in image_list:\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    outputs = pipe(\n",
    "        img,\n",
    "        prompt=prompt,\n",
    "        generate_kwargs={\"max_new_tokens\": 200}\n",
    "    )\n",
    "    \n",
    "    response = outputs[0]['generated_text'].split('\\nASSISTANT: ')[1]\n",
    "    \n",
    "    print(response)\n",
    "    \n",
    "    instruction = f\"次の文章を日本語に翻訳してください。\\n{response}\"    \n",
    "    client = OpenAI()\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": instruction},\n",
    "      ]\n",
    "    )\n",
    "    \n",
    "    response = response.choices[0].message.content\n",
    "    \n",
    "    print(response)\n",
    "    print('\\n==========\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe660e2-819c-40a9-b9a0-85caef016e38",
   "metadata": {},
   "source": [
    "# バッチ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643a7b15-c581-42ea-aaa8-b239137145d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
