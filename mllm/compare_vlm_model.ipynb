{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfc42895-1f0c-463e-8ac1-e7b57819cc89",
   "metadata": {},
   "source": [
    "# VLMモデル比較\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbdad86-9375-4af9-baa0-1c91d54701cb",
   "metadata": {},
   "source": [
    "### 共通のimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d2ed89-4a1f-4e30-ae7c-f10907030a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共通のimport\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "from PIL import Image, ImageDraw\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from src.utils import download_image_from_s3\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForVision2Seq, AutoImageProcessor\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "\n",
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01f804b-b5a0-4b31-972b-f05a65184f42",
   "metadata": {},
   "source": [
    "### 検証用の画像を準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515a2112-c4f5-48d8-b172-46f667f6d6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('/media/sj-archimedes/data/share/OddAI_Library_practice/data08/fba_test_20231001-20231031.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e29d65-49f0-4527-846d-51768922a50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.query('creative_type == \"image\"')\n",
    "df = df[['creative_media_hash', 'creative_media_url', 'headline', 'description', 'genre']]\n",
    "df['creative_media_hash'] = df['creative_media_hash'].map(lambda x: x[0])\n",
    "df['creative_media_url'] = df['creative_media_url'].map(lambda x: x[0])\n",
    "df['headline'] = df['headline'].map(lambda x: x[0])\n",
    "df['description'] = df['description'].map(lambda x: x[0])\n",
    "df = df.drop_duplicates('creative_media_hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c562186f-653e-48ab-a281-bb044df818ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d12a25-0a1a-4e0e-bf93-7ee3a12bc16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_genres = ['FMCG（日用消費財）', 'ヘルス&ビューティ', '人材', '教育・資格', 'ショッピング']\n",
    "\n",
    "sample_image_dict = {}\n",
    "for genre in target_genres:\n",
    "    _df = df.query('genre == @genre')\n",
    "    sample_image_dict[genre] = [download_image_from_s3(s3_url) for s3_url in _df['creative_media_url'].sample(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a56af-2a09-4f0b-9d18-7bf87eaa881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./sample_image/sample_image_dict.pkl', 'wb') as w:\n",
    "#     pickle.dump(sample_image_dict, w)\n",
    "\n",
    "with open('./sample_image/sample_image_dict.pkl', 'rb') as r:\n",
    "    sample_image_dict = pickle.load(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477a23ea-e75c-4199-a97b-0de68ae6b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for genre, img in sample_image_dict.items():\n",
    "    fig, ax = plt.subplots(1,3,figsize=(15,5))\n",
    "    for i in range(3):\n",
    "        ax[i].imshow(img[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a6e23b-6d04-4f1c-8399-f87ce6cc7eba",
   "metadata": {},
   "source": [
    "## stabilityai/japanese-stable-vlm\n",
    "\n",
    "- 商用利用可で日本語対応のVLMモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62968f2-b63f-45c6-90e4-a7df2e2584d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"/media/sj-archimedes/data/03_pretrained_model/llm/stabilityai/japanese-stable-vlm\"\n",
    "model_kwargs = {\"trust_remote_code\": True, \"low_cpu_mem_usage\": True}\n",
    "model_kwargs[\"variant\"] = \"fp16\"\n",
    "model_kwargs[\"torch_dtype\"] = torch.float16\n",
    "model_kwargs[\"device_map\"] = \"auto\"\n",
    "\n",
    "model = AutoModelForVision2Seq.from_pretrained(model_id, **model_kwargs)\n",
    "processor = AutoImageProcessor.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e9613c-baba-4db4-ad89-f2b0e2744128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_vlm_caption(image_list, prompt):\n",
    "    fig, ax = plt.subplots(1,3,figsize=(15,5))\n",
    "    for i in range(3):\n",
    "        ax[i].imshow(image_list[i])\n",
    "    plt.show()\n",
    "\n",
    "    caption_result = []\n",
    "    for img in image_list:\n",
    "        # 入力の準備\n",
    "        inputs = processor(images=img, return_tensors=\"pt\")\n",
    "        text_encoding = tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "        inputs.update(text_encoding)\n",
    "        \n",
    "        # 推論の実行\n",
    "        outputs = model.generate(\n",
    "            **inputs.to(device=model.device),\n",
    "            do_sample=True,\n",
    "            num_beams=1,\n",
    "            max_new_tokens=512,\n",
    "            min_length=1,\n",
    "            repetition_penalty=1.5,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "        caption_result.append(generated_text)\n",
    "    return caption_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ed4aa-3d7e-4605-9bb7-c5dc22bc5aca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。\n",
    "\n",
    "### 指示: \n",
    "画像を詳細に述べてください。\n",
    "\n",
    "### 応答: \n",
    "\"\"\"\n",
    "\n",
    "for genre, img in sample_image_dict.items():\n",
    "    caption_list = stable_vlm_caption(img, prompt)\n",
    "    for caption in caption_list:\n",
    "        print(caption)\n",
    "        print('\\n=====\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5f45e5-65fc-48d8-9ce5-e371e162fc8f",
   "metadata": {},
   "source": [
    "- 単純な画像を説明させるようなキャプション生成は全然だめ\n",
    "- 下記のようにVQAの使い方もできるようなので、質問形式にしてみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa073d-53a6-45bd-9e9b-ec534a9d667f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。\n",
    "\n",
    "### 指示: \n",
    "与えられた画像を下に、質問に答えてください。\n",
    "人はいますか？\n",
    "\n",
    "### 応答: \n",
    "\"\"\"\n",
    "\n",
    "for genre, img in sample_image_dict.items():\n",
    "    caption_list = stable_vlm_caption(img, prompt)\n",
    "    for caption in caption_list:\n",
    "        print(caption)\n",
    "        print('\\n=====\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee45b0ed-1023-4342-81fb-e5c3d4c150b1",
   "metadata": {},
   "source": [
    "- 答えられたり、答えられなかったり。微妙\n",
    "- yes/noで回答してくれたほうが後々便利なので、試す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ab4ffc-8d09-4c84-b3ed-85e974760c59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。\n",
    "\n",
    "### 指示: \n",
    "与えられた画像を下に、質問に答えてください。\n",
    "人はいますか？はい/いいえ で答えてください。\n",
    "\n",
    "### 応答: \n",
    "\"\"\"\n",
    "\n",
    "for genre, img in sample_image_dict.items():\n",
    "    caption_list = stable_vlm_caption(img, prompt)\n",
    "    for caption in caption_list:\n",
    "        print(caption)\n",
    "        print('\\n=====\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9da9d2-c680-46a3-af20-ac97060948fe",
   "metadata": {},
   "source": [
    "- はいかいいえで答えてくれ、という指示に従うことがそもそも難しそう。\n",
    "- ただある程度回答はできている\n",
    "- もう少し広告ならではの質問をしてみる。訴求観点を問う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e509e6-7663-4644-ad52-f9d4293ecc1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。\n",
    "\n",
    "### 指示: \n",
    "与えられた画像を下に、質問に答えてください。\n",
    "割引や特典を促す広告ですか？\n",
    "\n",
    "### 応答: \n",
    "\"\"\"\n",
    "\n",
    "for genre, img in sample_image_dict.items():\n",
    "    caption_list = stable_vlm_caption(img, prompt)\n",
    "    for caption in caption_list:\n",
    "        print(caption)\n",
    "        print('\\n=====\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62f54e6-d12e-4617-8217-61d8a8bbe5a9",
   "metadata": {},
   "source": [
    "- ある程度答えられているような、でも精度は悪い\n",
    "- もう少し定性的な観点を聞いてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4834aec6-1984-4e50-8d31-57472dfe0752",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。\n",
    "\n",
    "### 指示: \n",
    "与えられた画像を下に、質問に答えてください。\n",
    "この広告画像の魅力な点を教えてください。\n",
    "\n",
    "### 応答: \n",
    "\"\"\"\n",
    "\n",
    "for genre, img in sample_image_dict.items():\n",
    "    caption_list = stable_vlm_caption(img, prompt)\n",
    "    for caption in caption_list:\n",
    "        print(caption)\n",
    "        print('\\n=====\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e5ded0-7909-4f3a-99e5-11db40cc5865",
   "metadata": {},
   "source": [
    "- たまにいい感じに答えられたりしているが、基本的に全然ダメ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7b58e7-9b91-4d06-b581-febd3c413d31",
   "metadata": {},
   "source": [
    "## Salesforce/blip2-opt-2.7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bed4381-8f9e-46a4-b1f5-f54473ecf22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = '/media/sj-archimedes/data/03_pretrained_model/llm/salesforce/blip2-opt-2.7b'\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(model_id, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fb8008-8c40-4cdf-91af-3e97162de3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blip2_caption(image_list, prompt=None):\n",
    "    fig, ax = plt.subplots(1,3,figsize=(15,5))\n",
    "    for i in range(3):\n",
    "        ax[i].imshow(image_list[i])\n",
    "    plt.show()\n",
    "\n",
    "    caption_result = []\n",
    "    for img in image_list:\n",
    "        # 入力の準備\n",
    "        inputs = processor(img, text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        caption_result.append(generated_text)\n",
    "    return caption_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d49e51e-dd3f-4b1a-a5c7-f58ac14a7ad1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for genre, img in sample_image_dict.items():\n",
    "    caption_list = blip2_caption(img)\n",
    "    for caption in caption_list:\n",
    "        print(caption)\n",
    "        print('\\n=====\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f8ae5-b6a7-4a38-aeac-01fa5a0925e8",
   "metadata": {},
   "source": [
    "- ある程度写っているものを捉えていることはできている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9780a3f5-1c3c-40f4-a6c7-0f3744f70957",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = \"Question: Is there anyone there? Answer:\"\n",
    "\n",
    "for genre, img in sample_image_dict.items():\n",
    "    caption_list = blip2_caption(img, prompt)\n",
    "    for caption in caption_list:\n",
    "        print(caption)\n",
    "        print('\\n=====\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f3bd17-c2de-4ecf-8e52-225bf179f4f8",
   "metadata": {},
   "source": [
    "- 人がいるかどうかを尋ねる簡単な質問でも全然答えられていない。\n",
    "- やはり広告画像をBLIP-2で扱うのは難しい、と思われる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256e7b23-faa2-43ff-bc7e-9e1ed98dafc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = \"Question: Does it include content promoting discounts or benefits? Answer:\"\n",
    "\n",
    "for genre, img in sample_image_dict.items():\n",
    "    caption_list = blip2_caption(img, prompt)\n",
    "    for caption in caption_list:\n",
    "        print(caption)\n",
    "        print('\\n=====\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13708bc7-dac8-4e8f-a023-a78acf56ce1f",
   "metadata": {},
   "source": [
    "- すべてにYesと答えてしまった。ちょっとBLIP-2は難しそうだ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644be9d5-4771-43a9-8a2a-7116342886eb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## GRiT\n",
    "\n",
    "- https://zenn.dev/turing_motors/articles/ai-movie-searcher#grit\n",
    "- こちらの記事で紹介されているように、detection & captioninigを同時に行うモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a37e5f1-1ced-4a25-8cc0-bc6b88c84890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80652b3f-145f-49e4-8156-e7ca594e9510",
   "metadata": {},
   "source": [
    "## DETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbcf75c-fadf-4fa6-95eb-cedcb633bc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習済FeatureExtractorをロード\n",
    "from layout_detr.src.layout_detr import LayoutDetrFeatureExtractor\n",
    "from layout_detr.src.layout_detr import LayoutDetrForObjectDetection\n",
    "processor = LayoutDetrFeatureExtractor()\n",
    "model = LayoutDetrForObjectDetection.from_pretrained('v1-nlabel-1')\n",
    "model.to('cuda:0')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74369464-0c1d-4f21-800e-14d6127f7e27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = sample_image_dict['人材'][0]\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b3c77-bb20-4eae-b1f1-5835250e59ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor.preprocess(img, return_tensors='pt')\n",
    "inputs = {k:v.to(model.device) for k, v in inputs.items()}\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41fa3e1-d662-4faf-8bdf-ad005c0afa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = torch.Tensor([img.size[::-1]])  # 縦横反転\n",
    "postprocessed = processor.post_process_object_detection(outputs, target_sizes=target_size, threshold=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3ce7f3-ef91-4aef-952b-0d39d1321b1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = postprocessed['scores'].cpu().detach().numpy()\n",
    "boxes = postprocessed['boxes'].cpu().detach().numpy()\n",
    "\n",
    "N_BOXES = 8\n",
    "ids = scores.argsort()[::-1][:N_BOXES]  # (１つの式にスライスとストライドを同時に使わない. by Effective Python)\n",
    "ids = ids[:6]\n",
    "scores = scores[ids]\n",
    "boxes = boxes[ids]\n",
    "\n",
    "# drawer = ImageDraw.Draw(img)\n",
    "# for box in boxes:\n",
    "#     drawer.rectangle(box, outline='red', width=3)\n",
    "# img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f29f4bf-89c8-405b-8166-b71ea0364aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for box in boxes:\n",
    "    display(img.crop(box))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743235f4-5ea0-4a5b-8742-3acccdc78021",
   "metadata": {},
   "source": [
    "## LLaVA-1.5-7b\n",
    "- 商用利用はできないけど、OSSでは最強クラスのモデル\n",
    "- 日本語で出力してもらうようにプロンプトを書けば日本語で生成してくれるけど、英語で生成するときよりも性能下がるので、基本的に英語ベースでの使用を前提\n",
    "- そのため、出力結果をGPT-3.5で翻訳する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86080d8-eeea-449d-b1c7-6c610a310686",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "model_id = \"/media/sj-archimedes/data/03_pretrained_model/llm/llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "model = pipeline(\n",
    "    \"image-to-text\",\n",
    "    model=model_id,\n",
    "    device_map='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0540d1f-8c62-42d3-898d-e36dfa8b6f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llava_caption(image_list, prompt, chatgpt=True):\n",
    "    fig, ax = plt.subplots(1,3,figsize=(15,5))\n",
    "    for i in range(3):\n",
    "        ax[i].imshow(image_list[i])\n",
    "    plt.show()\n",
    "\n",
    "    for img in image_list:\n",
    "        outputs = model(\n",
    "            img,\n",
    "            prompt=prompt,\n",
    "            generate_kwargs={\"max_new_tokens\": 512}\n",
    "        )\n",
    "        \n",
    "        response = outputs[0]['generated_text'].split('\\nASSISTANT: ')[1]\n",
    "        \n",
    "        print(response)\n",
    "\n",
    "        if chatgpt:\n",
    "            instruction = f\"次の文章を日本語に翻訳してください。\\n{response}\"\n",
    "            \n",
    "            \n",
    "            client = OpenAI()\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "              model=\"gpt-3.5-turbo\",\n",
    "              messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": instruction},\n",
    "              ]\n",
    "            )\n",
    "            \n",
    "            response = response.choices[0].message.content\n",
    "            \n",
    "            print(response)\n",
    "        print('\\n==========\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971afced-e632-447e-8d6f-15b5919a34ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prompt = \"USER: <image>\\nPlease explain in detail how this advertisement image has been designed to appear attractive to consumers.\\nASSISTANT:\"\n",
    "prompt = \"USER: <image>\\nPlease describe this image.\\nASSISTANT:\"\n",
    "for genre, image_list in sample_image_dict.items():\n",
    "    llava_caption(image_list, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8280a713-3c9d-499d-b353-f2d4031a95e2",
   "metadata": {},
   "source": [
    "- ちらほら間違いはあるけど、基本的な情報はちゃんと認識できている。\n",
    "- 以下の例は広告画像の訴求内容やデザイン的な工夫を説明させるプロンプトを投げている\n",
    "- 詳しく説明しろ、といったからなのか、単純に画像を説明してくれ、というときより、生成されるテキストが長くなる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6c4ca2-2a23-4b3c-95b1-ff9379f516ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# この広告画像がどのようにして消費者に魅力的に見えるようにデザインされているか、詳しく説明してください。\n",
    "prompt = \"USER: <image>\\nPlease explain in detail how this advertisement image has been designed to appear attractive to consumers.\\nASSISTANT:\"\n",
    "# prompt = \"USER: <image>\\nPlease describe this image.\\nASSISTANT:\"\n",
    "for genre, image_list in sample_image_dict.items():\n",
    "    llava_caption(image_list, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e02599c-cfc7-490a-ba56-114fa50cf74e",
   "metadata": {},
   "source": [
    "- 結構いい感じに見える\n",
    "- yes or no の使い方も一応やってみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3656fc-cd46-4efe-801c-92b2621fdb35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = \"USER: <image>\\nIs there a person in this image? Please answer yes or no.\\nASSISTANT:\"\n",
    "for genre, image_list in sample_image_dict.items():\n",
    "    llava_caption(image_list, prompt, chatgpt=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ceebbb-1e57-4a06-b3db-789d169e3021",
   "metadata": {},
   "source": [
    "- 完璧\n",
    "- 手だけ写っているものもYesと答えるけど、この辺はプロンプトで制御できそう\n",
    "- 一度に複数のタグをyes noで答えられるかもやってみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abcacf0-92d8-48b8-a450-23d220d12908",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 画像をよく解析した上で、以下の質問にはいかいいえでこたえてください。\n",
    "# 1. 人は写っていますか？\n",
    "# 2. 屋外ですか？\n",
    "# 3. 動物がいますか？\n",
    "# 4. 中央にキャッチコピーが配置されていますか？\n",
    "# 5. 画像が分割されたデザインになっていますか？\n",
    "prompt = \"\"\"USER: <image>\\nAfter carefully analyzing the image, answer the following questions with a yes or no:\n",
    "1. Is there a person in the picture?\n",
    "2. Is it outdoors?\n",
    "3. Are there any animals?\n",
    "4. Is there a catchphrase placed in the center?\n",
    "5. Is the image designed in a split layout?\n",
    "ASSISTANT:\"\"\"\n",
    "for genre, image_list in sample_image_dict.items():\n",
    "    llava_caption(image_list, prompt, chatgpt=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9ba41f-2a69-47ae-9306-9a5c803ea4f1",
   "metadata": {},
   "source": [
    "- 複数同時に回答するのは無理らしい\n",
    "- ただし、はいかいいえで回答する制限をなくしたら、ちゃんと箇条書きで答えてくれる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e0aa7f-58b2-49a9-b30c-6f7f5107b916",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 画像をよく解析した上で、以下の質問に答えてください。\n",
    "# 1. 人は写っていますか？\n",
    "# 2. 屋外ですか？\n",
    "# 3. 動物がいますか？\n",
    "# 4. 中央にキャッチコピーが配置されていますか？\n",
    "# 5. 画像が分割されたデザインになっていますか？\n",
    "prompt = \"\"\"USER: <image>\\nAfter carefully analyzing the image, answer the following questions:\n",
    "1. Is there a person in the picture?\n",
    "2. Is it outdoors?\n",
    "3. Are there any animals?\n",
    "4. Is there a catchphrase placed in the center?\n",
    "5. Is the image designed in a split layout?\n",
    "ASSISTANT:\"\"\"\n",
    "for genre, image_list in sample_image_dict.items():\n",
    "    llava_caption(image_list, prompt, chatgpt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4704bdb6-a1d0-487d-945c-cc65b779ea92",
   "metadata": {},
   "source": [
    "- 箇条書きで回答はしてくれるようになった\n",
    "- 答えられる質問とそうでない質問が明確になった\n",
    "    - 広告画像のレイアウト的な質問（No.4,5は難しい）が、No.1,2,3のような一般的な写真に何が写っているかやその状況を問うような質問は得意そう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3e4c05-6f85-4e81-a106-5d67498ef7b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = \"USER: <image>\\nDoes it include content promoting discounts or benefits?\\nASSISTANT:\"\n",
    "for genre, image_list in sample_image_dict.items():\n",
    "    llava_caption(image_list, prompt, chatgpt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4a24b8-2ac9-436b-9de6-6461a378db86",
   "metadata": {},
   "source": [
    "- 以外にも全然ダメだった。Yesと答えやすいのかもしれない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a9635e-cbd9-44df-b07f-3909723de7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    ">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
