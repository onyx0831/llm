{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe063619-de24-407e-b066-37be856a513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lap_df = pd.read_pickle('/media/sj-archimedes/data/share/OddAI_Library_practice/data08/lap_trainval_20221001-20231031.pkl')\n",
    "lap_df = lap_df.query('creative_type == \"image\"')\n",
    "lap_df['creative_media_url'] = lap_df['creative_media_url'].map(lambda x: x[0])\n",
    "lap_df['creative_media_hash'] = lap_df['creative_media_hash'].map(lambda x: x[0])\n",
    "df = lap_df\n",
    "\n",
    "all_caption_dict = {}\n",
    "\n",
    "caption_dict_list = glob('/media/sj-archimedes/data/masaya_kondo/research/mllm/caption/llava1.5-7b_caption/*.pkl')\n",
    "caption_dict = {}\n",
    "for fname in caption_dict_list:\n",
    "    with open(fname, 'rb') as r:\n",
    "        caption_dict |= pickle.load(r)\n",
    "all_caption_dict['caption'] = caption_dict\n",
    "\n",
    "appeal_dict_list = glob('/media/sj-archimedes/data/masaya_kondo/research/mllm/caption/llava1.5-7b_appeal_caption/*.pkl')\n",
    "appeal_dict = {}\n",
    "for fname in appeal_dict_list:\n",
    "    with open(fname, 'rb') as r:\n",
    "        appeal_dict |= pickle.load(r)\n",
    "all_caption_dict['appeal'] = appeal_dict\n",
    "\n",
    "detr_dict_list = glob('/media/sj-archimedes/data/masaya_kondo/research/mllm/caption/detr_blip2_caption/*.pkl')\n",
    "detr_dict = {}\n",
    "for fname in detr_dict_list:\n",
    "    with open(fname, 'rb') as r:\n",
    "        detr_dict |= pickle.load(r)\n",
    "detr_dict = {k:'. '.join(v) for k, v in detr_dict.items()}\n",
    "all_caption_dict['detr_blip2'] = detr_dict\n",
    "\n",
    "df['caption'] = df['creative_media_hash'].map(\n",
    "    lambda x: all_caption_dict['caption'][x] if x in all_caption_dict['caption'] else None\n",
    ")\n",
    "df['appeal'] = df['creative_media_hash'].map(\n",
    "    lambda x: all_caption_dict['appeal'][x] if x in all_caption_dict['appeal'] else None\n",
    ")\n",
    "df['detr_blip2'] = df['creative_media_hash'].map(\n",
    "    lambda x: all_caption_dict['detr_blip2'][x] if x in all_caption_dict['detr_blip2'] else None\n",
    ")\n",
    "df = df[~df['caption'].isna()]\n",
    "\n",
    "df = df[['creative_media_hash', 'creative_media_url', 'caption', 'appeal', 'detr_blip2']]\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# df['embeddings_caption'] = df['caption'].progress_apply(\n",
    "#     lambda x: _ctx_encoder(**_ctx_tokenizer(x, return_tensors='pt', max_length=512).to(device))[0][0].cpu().numpy()\n",
    "# )\n",
    "# df['embeddings_appeal'] = df['appeal'].progress_apply(\n",
    "#     lambda x: _ctx_encoder(**_ctx_tokenizer(x, return_tensors='pt', max_length=512).to(device))[0][0].cpu().numpy()\n",
    "# )\n",
    "# df['embeddings_detr_blip2'] = df['detr_blip2'].progress_apply(\n",
    "#     lambda x: _ctx_encoder(**_ctx_tokenizer(x, return_tensors='pt', max_length=512).to(device))[0][0].cpu().numpy()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8bc7e4-256d-429a-a4ea-7fbdae9a05f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "\n",
    "device = 'cuda:0'\n",
    "def load_query_model():\n",
    "    ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "    ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "    q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "    q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "    ctx_encoder.to(device)\n",
    "    return ctx_encoder, ctx_tokenizer, q_encoder, q_tokenizer\n",
    "\n",
    "ctx_encoder, ctx_tokenizer, q_encoder, q_tokenizer = load_query_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae4211a-2d9c-4c05-8563-a769204ae730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea7f88d-6181-48f3-b37e-714ae670384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=16)\n",
    "\n",
    "all_outputs = []\n",
    "for batch in tqdm(loader):\n",
    "    inputs = ctx_tokenizer(batch['caption'], padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "    inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "    outputs = ctx_encoder(**inputs)\n",
    "    outputs = outputs.pooler_output.detach().cpu().tolist()\n",
    "    all_outputs += outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8bccad-0f27-4f46-82a2-13e66fe3cee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df['hoge'] = all_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0307d73f-e207-4bb1-b960-b1200c22f3ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95f32cd-a7ab-41c8-a9d2-ac4f3525f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共通のimport\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "from PIL import Image, ImageDraw\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from src.utils import download_image_from_s3\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForVision2Seq, AutoImageProcessor\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "from layout_detr.src.layout_detr import LayoutDetrFeatureExtractor\n",
    "from layout_detr.src.layout_detr import LayoutDetrForObjectDetection\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "with open('./sample_image/sample_image_dict.pkl', 'rb') as r:\n",
    "    sample_image_dict = pickle.load(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90bd08a-ecaa-4b4c-ad1a-b1b061c01dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for genre, img in sample_image_dict.items():\n",
    "    print(genre)\n",
    "    fig, ax = plt.subplots(1,3,figsize=(15,5))\n",
    "    for i in range(3):\n",
    "        ax[i].imshow(img[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078ed875-9970-49d1-aa8e-c0d2a507ecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "detor_processor = LayoutDetrFeatureExtractor()\n",
    "detor_model = LayoutDetrForObjectDetection.from_pretrained('v1-nlabel-1')\n",
    "detor_model.to('cuda:0')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d903cd-99e2-4924-ab99-4d7ce49581fc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# BOX検知 調査"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ce266-f088-4129-ac93-40735ecb1361",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "genre = '教育・資格'\n",
    "img = sample_image_dict[genre][idx].copy()\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "inputs = processor.preprocess(img, return_tensors='pt')\n",
    "inputs = {k:v.to(detor_model.device) for k, v in inputs.items()}\n",
    "outputs = detor_model(**inputs)\n",
    "target_size = torch.Tensor([img.size[::-1]])  # 縦横反転\n",
    "postprocessed = processor.post_process_object_detection(outputs, target_sizes=target_size, threshold=0)[0]\n",
    "scores = postprocessed['scores'].cpu().detach().numpy()\n",
    "boxes = postprocessed['boxes'].cpu().detach().numpy()\n",
    "N_BOXES = 10\n",
    "ids = scores.argsort()[::-1][:N_BOXES]\n",
    "scores = scores[ids]\n",
    "boxes = boxes[ids]\n",
    "drawer = ImageDraw.Draw(img)\n",
    "for box in boxes:\n",
    "    drawer.rectangle(box, outline='red', width=3)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "img = sample_image_dict[genre][idx].copy()\n",
    "num_ax = math.ceil(len(boxes)/5)\n",
    "plt.subplots(num_ax, 5)\n",
    "for i, box in enumerate(boxes):\n",
    "    plt.subplot(num_ax, 5, i+1)\n",
    "    plt.imshow(img.crop(box))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6216257-47d0-43bf-9187-88ec81b0a5f0",
   "metadata": {},
   "source": [
    "# BOX 取得\n",
    "- サンプルデータ的にはBOXの取得数は10がよさそう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d57a74-78fe-42ba-b425-3bdb0d498b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_box_image_dict = {}\n",
    "for genre, image_list in sample_image_dict.items():\n",
    "    for i, img in enumerate(image_list):\n",
    "        inputs = detor_processor.preprocess(img, return_tensors='pt')\n",
    "        inputs = {k:v.to(detor_model.device) for k, v in inputs.items()}\n",
    "        outputs = detor_model(**inputs)\n",
    "        target_size = torch.Tensor([img.size[::-1]])  # 縦横反転\n",
    "        postprocessed = detor_processor.post_process_object_detection(outputs, target_sizes=target_size, threshold=0)[0]\n",
    "        scores = postprocessed['scores'].cpu().detach().numpy()\n",
    "        boxes = postprocessed['boxes'].cpu().detach().numpy()\n",
    "        N_BOXES = 10\n",
    "        ids = scores.argsort()[::-1][:N_BOXES]  # (１つの式にスライスとストライドを同時に使わない. by Effective Python)\n",
    "        # ids = ids[:6]\n",
    "        scores = scores[ids]\n",
    "        boxes = boxes[ids]\n",
    "        box_images = [img.crop(box) for box in boxes]\n",
    "        sample_box_image_dict[f'{genre}_{i}'] = box_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f914918e-3e1d-4aeb-9c35-8b6f03c43625",
   "metadata": {},
   "source": [
    "# BLIP-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff6ba09-b066-4bcc-88e5-570d39387b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = '/media/sj-archimedes/data/03_pretrained_model/llm/salesforce/blip2-opt-2.7b'\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(model_id, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d8050c-6300-43dd-91b1-796a64edfbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blip2_caption(image_list, prompt=None):\n",
    "    num_images = len(image_list)\n",
    "    fig, ax = plt.subplots(1, num_images,figsize=(20,5))\n",
    "    for i in range(num_images):\n",
    "        ax[i].imshow(image_list[i])\n",
    "    plt.show()\n",
    "\n",
    "    caption_result = []\n",
    "    for img in image_list:\n",
    "        # 入力の準備\n",
    "        inputs = processor(img, text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        caption_result.append(generated_text)\n",
    "    return caption_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6156f734-fa45-477c-94ae-d6a3cad4d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for genre, image_list in sample_box_image_dict.items():\n",
    "    caption_list = blip2_caption(image_list)\n",
    "    for caption in caption_list:\n",
    "        print(caption)\n",
    "        print('\\n=====\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d44d1f-b2e7-46a2-aab7-691c8c3fa9db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = \"Question: Is this a character (or letter)? .Answer:\"\n",
    "\n",
    "for genre, image_list in sample_box_image_dict.items():\n",
    "    caption_list = blip2_caption(image_list, prompt=prompt)\n",
    "    for caption in caption_list:\n",
    "        print(caption)\n",
    "        print('\\n=====\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3714a9-9f2d-463f-a65c-070aaff6289e",
   "metadata": {},
   "source": [
    "# japanese_stable_vlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd458d4-9b0d-40d8-92b1-114d4e8e665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"/media/sj-archimedes/data/03_pretrained_model/llm/stabilityai/japanese-stable-vlm\"\n",
    "model_kwargs = {\"trust_remote_code\": True, \"low_cpu_mem_usage\": True}\n",
    "model_kwargs[\"variant\"] = \"fp16\"\n",
    "model_kwargs[\"torch_dtype\"] = torch.float16\n",
    "model_kwargs[\"device_map\"] = \"auto\"\n",
    "\n",
    "model = AutoModelForVision2Seq.from_pretrained(model_id, **model_kwargs)\n",
    "processor = AutoImageProcessor.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da9fd92-14ba-49de-89a7-f0f7c2fd5f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_vlm_caption(image_list, prompt):\n",
    "    num_images = len(image_list)\n",
    "    fig, ax = plt.subplots(1, num_images,figsize=(20,5))\n",
    "    for i in range(num_images):\n",
    "        ax[i].imshow(image_list[i])\n",
    "    plt.show()\n",
    "\n",
    "    caption_result = []\n",
    "    for img in image_list:\n",
    "        # 入力の準備\n",
    "        inputs = processor(images=img, return_tensors=\"pt\")\n",
    "        text_encoding = tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "        inputs.update(text_encoding)\n",
    "        \n",
    "        # 推論の実行\n",
    "        outputs = model.generate(\n",
    "            **inputs.to(device=model.device),\n",
    "            do_sample=True,\n",
    "            num_beams=1,\n",
    "            max_new_tokens=512,\n",
    "            min_length=1,\n",
    "            repetition_penalty=1.5,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "        caption_result.append(generated_text)\n",
    "    return caption_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43722033-34a9-4c93-a137-2d36ad62cfab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。\n",
    "\n",
    "### 指示: \n",
    "画像を詳細に述べてください。\n",
    "\n",
    "### 応答: \n",
    "\"\"\"\n",
    "\n",
    "for genre, img in sample_box_image_dict.items():\n",
    "    caption_list = stable_vlm_caption(img, prompt)\n",
    "    for caption in caption_list:\n",
    "        print(caption)\n",
    "        print('\\n=====\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d623e60f-f6bc-4e15-b7d2-796f7eddbeae",
   "metadata": {},
   "source": [
    "# LLaVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25371a8-e73e-4789-87a2-60828288e576",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "model_id = \"/media/sj-archimedes/data/03_pretrained_model/llm/llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "model = pipeline(\n",
    "    \"image-to-text\",\n",
    "    model=model_id,\n",
    "    device_map='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e90013-1f00-451a-954b-a06180ff7894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llava_caption(image_list, prompt, chatgpt=True):\n",
    "    num_images = len(image_list)\n",
    "    fig, ax = plt.subplots(1, num_images,figsize=(20,5))\n",
    "    for i in range(num_images):\n",
    "        ax[i].imshow(image_list[i])\n",
    "    plt.show()\n",
    "\n",
    "    for img in image_list:\n",
    "        outputs = model(\n",
    "            img,\n",
    "            prompt=prompt,\n",
    "            generate_kwargs={\"max_new_tokens\": 512}\n",
    "        )\n",
    "        \n",
    "        response = outputs[0]['generated_text'].split('\\nASSISTANT: ')[1]\n",
    "        \n",
    "        print(response)\n",
    "\n",
    "        if chatgpt:\n",
    "            instruction = f\"次の文章を日本語に翻訳してください。\\n{response}\"\n",
    "            \n",
    "            \n",
    "            client = OpenAI()\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "              model=\"gpt-3.5-turbo\",\n",
    "              messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": instruction},\n",
    "              ]\n",
    "            )\n",
    "            \n",
    "            response = response.choices[0].message.content\n",
    "            \n",
    "            print(response)\n",
    "        print('\\n==========\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246ac4e3-0df9-4649-8ae6-014c8a14c1e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prompt = \"USER: <image>\\nPlease explain in detail how this advertisement image has been designed to appear attractive to consumers.\\nASSISTANT:\"\n",
    "prompt = \"USER: <image>\\nPlease describe this image.\\nASSISTANT:\"\n",
    "for genre, image_list in sample_box_image_dict.items():\n",
    "    llava_caption(image_list, prompt, chatgpt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739b557c-cdd3-4db7-9c31-87cf646e6a56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prompt = \"USER: <image>\\nPlease explain in detail how this advertisement image has been designed to appear attractive to consumers.\\nASSISTANT:\"\n",
    "prompt = \"\"\"USER: <image>\n",
    "This image is part of an advertisement. Please answer the following questions.\n",
    "1. Is it a text layer?\n",
    "2. What is written on it?\n",
    "3. Are there any products or items placed?\n",
    "4. Are there any people?\n",
    "5. What is depicted in the image?\"\n",
    "ASSISTANT:\"\"\"\n",
    "for genre, image_list in sample_box_image_dict.items():\n",
    "    llava_caption(image_list, prompt, chatgpt=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe5d47a-8595-43f4-a5af-c9929a51c204",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# スクリプト実装用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca93c3e-5930-4987-a38f-440efc1efa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./output_detr_blip2_caption\"\n",
    "target_medias = ['lap', 'fba']\n",
    "\n",
    "detor_processor = LayoutDetrFeatureExtractor()\n",
    "detor_model = LayoutDetrForObjectDetection.from_pretrained('v1-nlabel-1')\n",
    "detor_model.to('cuda:0')\n",
    "print()\n",
    "\n",
    "\n",
    "model_id = '/media/sj-archimedes/data/03_pretrained_model/llm/salesforce/blip2-opt-2.7b'\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(model_id, device_map='auto')\n",
    "\n",
    "\n",
    "def blip2_caption(image_list, prompt=None):\n",
    "    num_images = len(image_list)\n",
    "\n",
    "    caption_result = []\n",
    "    for img in image_list:\n",
    "        # 入力の準備\n",
    "        inputs = processor(img, text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        caption_result.append(generated_text)\n",
    "    return caption_result\n",
    "\n",
    "for media in target_medias:\n",
    "    print(media)\n",
    "    df = pd.read_pickle(f'/media/sj-archimedes/data/share/OddAI_Library_practice/data08/{media}_trainval_20221001-20231031.pkl')\n",
    "    df = df.query('creative_type == \"image\"')\n",
    "    df['creative_media_url'] = df['creative_media_url'].map(lambda x: x[0])\n",
    "    df['creative_media_hash'] = df['creative_media_hash'].map(lambda x: x[0])\n",
    "    df = df[['creative_media_url', 'creative_media_hash']]\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    caption_dict = {}\n",
    "    for i, row in tqdm(enumerate(df.itertuples()), total=df.shape[0]):\n",
    "        s3_url = row.creative_media_url\n",
    "        media_hash = row.creative_media_hash\n",
    "        if media_hash in caption_dict:\n",
    "            continue\n",
    "        img = download_image_from_s3(s3_url)\n",
    "        inputs = detor_processor.preprocess(img, return_tensors='pt')\n",
    "        inputs = {k:v.to(detor_model.device) for k, v in inputs.items()}\n",
    "        outputs = detor_model(**inputs)\n",
    "        target_size = torch.Tensor([img.size[::-1]])  # 縦横反転\n",
    "        postprocessed = detor_processor.post_process_object_detection(outputs, target_sizes=target_size, threshold=0)[0]\n",
    "        scores = postprocessed['scores'].cpu().detach().numpy()\n",
    "        boxes = postprocessed['boxes'].cpu().detach().numpy()\n",
    "        N_BOXES = 10\n",
    "        ids = scores.argsort()[::-1][:N_BOXES]  # (１つの式にスライスとストライドを同時に使わない. by Effective Python)\n",
    "        scores = scores[ids]\n",
    "        boxes = boxes[ids]\n",
    "        box_images = [img.crop(box) for box in boxes]\n",
    "        response = blip2_caption(box_images, prompt=None)\n",
    "\n",
    "        caption_dict[media_hash] = response\n",
    "\n",
    "        if len(caption_dict) == 10:\n",
    "            with open(f\"{output_dir}/{media}_{i}.pkl\", \"wb\") as w:\n",
    "                pickle.dump(caption_dict, w)\n",
    "            caption_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84786ca4-b943-481b-9f3b-ca3ebcad912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./output_detr_blip2_caption/lap_19.pkl', 'rb') as r:\n",
    "    outputs = pickle.load(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e7988-0fbb-4c04-ae1a-b216a1279059",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40216ef5-e038-4156-980e-890227a08d27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
